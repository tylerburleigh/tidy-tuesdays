
# Tidy Tuesday (2023-08-15): Spam mail

```{r}
library(tidyverse)
library(caret)
library(corrplot)
library(tidymodels)
library(usemodels)
library(future)
knitr::opts_chunk$set(echo = TRUE, fig.width = 4.5, fig.height = 2.5)
```

# Load data

```{r}
df <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-08-15/spam.csv') %>%
  mutate(yesno = factor(yesno == 'y', levels=c(TRUE, FALSE)))
```

# Data dictionary

```
variable  class       description
crl.tot   double      Total length of uninterrupted sequences of capitals
dollar    double 	    Occurrences of the dollar sign, as percent of total number of characters
bang 	    double 	    Occurrences of ‘!’, as percent of total number of characters
money 	  double 	    Occurrences of ‘money’, as percent of total number of characters
n000 	    double 	    Occurrences of the string ‘000’, as percent of total number of words
make 	    double 	    Occurrences of ‘make’, as a percent of total number of words
yesno 	  character 	Outcome variable, a factor with levels 'n' not spam, 'y' spam
```

The outcome variable is `yesno`, a character type, and all of the other variables are of a numeric type.

We can see that a lot of the feature engineering has already been done. It seems that whoever prepared this dataset has determined that spam emails can be meaningfully identified by:

- SHOUTING! (`crl.tot`, `bang`)
- and talk of making money (`make`, `money`, `dollar`) at values of 1,000 or greater (`n000`)

That definitely matches my experience reading spam!


# What can we do with this data?

How can we apply ML to this data, or what can we do with it? We can look at this as either a supervised or unsupervised ML problem. For supervised ML, we might be aiming to predict spam -- to develop a spam filter, for example. For unsupervised ML, we might be aiming to identify different kinds of mail within the spam and not-spam classes.

Let's start with supervised ML / prediction.


# Supervised ML: Predicting spam / spam filter

## Average values, by spam

Let's start by looking at some averages (mean and median), split by the outcome variable.

```{r}
df %>%
  group_by(yesno) %>%
  summarise_all(mean)
```

We can see that on average, spam emails have higher mean values for each of the predictors. No surprise there.

However, the medians of some variables are zero, which suggests those variables have heavily positively skewed distributions with many zero values (sometimes called "zero-inflation").

```{r}
df %>%
  group_by(yesno) %>%
  summarise_all(median)
```

We can confirm this by looking at the counts of zero values, in relation to the total counts.

As we see, the vast majority of the spam emails had non-zero values on these variables, and non-spam emails had significantly fewer non-zero values, with the exception of `crl.tot`. In particular, spam emails were MUCH more likely to contain "!", "$", "000", and "money".

```{r}
no = df %>% filter(yesno == FALSE) %>% select(-yesno)
yes = df %>% filter(yesno == TRUE) %>% select(-yesno)

round(colSums(no>0)/nrow(no)*100)
round(colSums(yes>0)/nrow(yes)*100)
```

## Distributions, by spam

Next, let's look at the distributions.

For these plots, since they're all have extreme skew, I'm going to truncate them at the 90th percentile and look at the left side where most of the mass is.

```{r, fig.width=4.5, fig.height=2.5}
for (c in c('crl.tot', 'dollar', 'bang', 'money', 'n000', 'make')){
  
  qtile_90 <- quantile(df[[c]], .90)
  
  df %>%
    filter(!!sym(c) < qtile_90) %>%
    ggplot(aes(x = !!sym(c), fill=yesno)) +
      geom_density(alpha=.7) +
      ggtitle(c) -> plot
  print(plot)
  
}
```

## Feature correlations

It doesn't look like the features are very strongly correlated. The strongest correlation is between `n000` and `dollar`, which is not particularly surprising since I would expect that "000" would tend to appear in the context of a dollar value like "$1000".

```{r}
df %>% 
  select(-yesno) %>%
  cor(use = "complete.obs")
```
If we convert the features to boolean, we can see that the presence of features have stronger correlations. The strongest correlation is again between `dollar` and `n000`, but `money` and `dollar` also occur together more often than not.

```{r}
df %>%
  select(-yesno, -crl.tot) %>%
  mutate(dollar = dollar > 0,
         bang = bang > 0,
         money = money > 0,
         n000 = n000 > 0,
         make = make > 0
  ) %>%
  cor(use = "complete.obs")
```

## Simple classification algorithm

Just for fun, let's see how well we can distinguish spam vs. not spam using a simple heuristic.

I'll label anything as spam if it contained at least 1 "money", "$", "000", and "!" OR if it contained more than 100 uninterrupted sequences of capital letters and at least 1 "1". This is just what comes to mind after looking at the frequency plots above.

```{r}
df %>%
  mutate(simple_spam_flag = factor((money > 0 & dollar > 0 & bang > 0 & n000 > 0) | 
                                     (crl.tot > 100 & bang > 0), 
                                   levels=c(TRUE, FALSE))
         ) -> df_flag
```

This simple classification algorithm achieved an accuracy of 79%. This doesn't seem too bad. But what is a good baseline of performance?

```{r}
confusionMatrix(df_flag$simple_spam_flag, df_flag$yesno, mode='everything')
```
We can see that the base rate of spam is 39%.

```{r}
mean(df$yesno == TRUE)
```

A good baseline model might be to predict the majority class, which in this case is not-spam.

This "always predict FALSE" baseline model can be expected to achieve 1 minus the base rate of spam (i.e., 61%), and we can see that this is the case if we construct just such a model. This tells us that the heuristic model above is quite a bit better than a completely naive model.

```{r}
confusionMatrix(factor(rep('FALSE',nrow(df_flag))), df_flag$yesno, mode='everything')
```

## Random forest

Next, I'll try a random forest model. This should perform much better than my simple heurstic model.

```{r}
split <- initial_split(df)
train <- training(split)
test <- testing(split)
cv <- vfold_cv(train)
```

I'll use `usemodels::use_ranger` to give me a starting template.

```{r}
use_ranger(yesno ~ ., train)
```

I'll remove the parameter tuning to keep things simple.

```{r}
ranger_recipe <- 
  recipe(formula = yesno ~ ., data = df)

ranger_spec <- 
  rand_forest(trees = 1000) %>% 
  set_mode("classification") %>% 
  set_engine("ranger") 

ranger_workflow <- 
  workflow() %>% 
  add_recipe(ranger_recipe) %>% 
  add_model(ranger_spec) 
```

Next, I'll fit the model using a resampling approach.

```{r}
set.seed(100)
plan(multisession)

fit_rf <- fit_resamples(
  ranger_workflow,
  cv,
  metrics = metric_set(accuracy, sens, spec, roc_auc),
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE,
                              extract = function(x) x)
)
```

Overall, accuracy is pretty good: 89% accuracy, 93% ROC-AUC.

```{r}
fit_rf %>%
  collect_metrics()
```

Next, we can check the performance on the test set.

We can use `collect_metrics()` function on the last fit.

```{r}
ranger_workflow %>%
  last_fit(split) %>% 
  collect_metrics()
```

Or we can use `confusionMatrix()` to get a bit more information.

```{r}
ranger_workflow %>%
  last_fit(split) %>% 
  extract_workflow() -> final_model

confusionMatrix(predict(final_model, test)$.pred_class, test$yesno, mode='everything', positive='TRUE')
```

Performance on the test set is similar to the training performance. Overall, accuracy is pretty good. It seems we're letting some spam mail through, which is why we see that sensitivity is somewhat low. For a spam detection filter, we'd want to bias towards minimizing false positives, and here we see that the specificity was quite good at ~95%.

# Unsupervised learning

Can we identify different types of spam mail? Intuitively, we might expect that some spam mail is about making money, and other spam mail is not.

I'll use kmeans clustering to identify two clusters.

```{r}
df %>% filter(yesno == TRUE) %>% select(-yesno) -> df_spam
kclust <- kmeans(df_spam, centers = 2)
kclust
```

```{r}
tidy(kclust)
```

Here we can see two clusters have emerged. The first cluster, which most spam mail was bucketed into, contained more bangs, fewer dollar signs, fewer 000s, fewer make, and fewer sequences of capital letters than the second cluster. Based on this, it would seem that the second cluster is more about making money than the first cluster.




